hocr_noodling
=============

This is part of an experiment to see if we can create semi-structured data from image-based pdfs. This component tries to do this from [hOCR](http://en.wikipedia.org/wiki/HOCR) files generated by [tesseract](http://code.google.com/p/tesseract-ocr/). 

HOCR outputs bounding boxes for words (and a whole hierarchy of sections of text; see more [here](https://docs.google.com/a/sunlightfoundation.com/document/d/1QQnIQtvdAC_8n92-LhwPcjtAUFwBlzE8EWnKAxlgVf0/preview).) This experiment treats word bounding boxes as polygons, and provides utilities for inserting them in bulk to postgis/postgres. The idea is that page classifiers can be built on the basis of known word positions, and then applied to pages to extract the data sought. Utilities are also included to return pages as an array of words with [GEOS](http://trac.osgeo.org/geos/)-style polygons (we're using [django's bindings to GEOS](https://docs.djangoproject.com/en/dev/ref/contrib/gis/geos/), so the shapes are actually GEOSGeometry objects).


##### A few quirks:
We're assuming that hOCR arrives as latin-1 *despite* being labeled as utf-8. That's because the test data came from a version of tesseract with [this bug](https://groups.google.com/forum/#!topic/tesseract-ocr/UiyIMUWMzsU).

We're assuming the precise syntax generated by tesseract--other hOCR flavors may break this.

Because we're (ab)using postGIS's spatial model, one needs to remove any checks that our data actually fits into any known srid. At some point I should roll a custom srid, but so far it sorta works as long as you remove the constraint like this (assuming django's default naming)

	alter table documents_pageword drop constraint "enforce_srid_bbox";
	

Requirements
============

Running the whole demo requires django running with a postgres database with postgis installed and working, as well as a variety of python libraries (see requirements.txt)

That said, the hocr page parser ( in hocr_util/parser/ ) should work without django/postgis -- see demo.py in that directory. At the moment, the stuff in display/ are something like first steps towards a raphael-based visualization of documents based on single-page hocr pages. It will currently display the location of word bounding boxes and the words if you hover on them, but not much more than that. There's a demo [here](http://jacobfenton.s3.amazonaws.com/hocr/display_hocr_page.html). 

Loading Test Data
===============

Sample hOCR documents are in the  /hocr_util/parser/test_hocr/ directory. (For reference the pdf documents they were parsed from are in /hocr_util/parser/test_pdf/ ).

Assuming all the requirements are there and syncdb has been run, and the troublesome database constraints habe been purged, you should be able to insert a document with 

	manage.py test_load
The command is in /documents/management/commands/ ; this isn't meant as a fuller loader--the file path is hardcoded--but an example of how to load. To get a representation of a page with geometries included, see /documents/management/commands/test_word_shapes.py.

Envisioned Operation
============
This is, of course, experimental software, so we have no idea how it should work. But the envisioned use case is for a situation where there are millions of pages of hOCR data and dozens of different form submissions with both well-defined and open-ended fields. So that would likely entail training / testing and extraction. 

During the training / testing phase a substantial subset of the data is loaded into a postgis database and classifiers / extractors are written. The database is partially an investigative tool to be used interactively--show me all pages where there's a word near this position on the page that is 'Form' or something like that. Once reliable classifiers and extractors are written, the hope is that they could be applied one page at a time outside of a database context, thus alleviating the need to enter all of them into a database. 

One note about database entry: the routines included here are relatively efficient--they enter each page's worth of words in a single bulk copy statement--but constraints or indexes can slow this down substantially. Ultimately having one item per word is a fairly inefficient way of representing a large corpus, soâ€¦ 

In extraction mode, pages would still be processed in the same way--an array of words with GEOSgeometry objects would returned. But the extractors and classifiers would have to be written to run on GEOSGeometries, not against postgis. There are, of course, queries possible in postgis that may not be supported by the geodjango geos api. 


	
